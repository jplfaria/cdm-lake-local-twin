{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontology Term Validation and Querying\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Check if a list of ontology terms exists in the CDM ontology tables\n",
    "2. Query OMP (Ontology of Microbial Phenotypes) terms\n",
    "3. Query ECO (Evidence and Conclusion Ontology) terms\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from spark.utils import get_spark_session\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = get_spark_session()\n",
    "namespace = 'ontology_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Term Existence Validation\n",
    "\n",
    "This function takes a list of ontology terms and checks which ones exist in the CDM ontology tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ontology_terms(term_list: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Validate a list of ontology terms against the CDM ontology database.\n",
    "    \n",
    "    Args:\n",
    "        term_list: Comma-separated string of ontology terms (e.g., \"OMP:0006023, ECO:0006056\")\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (found_terms_df, missing_terms_df)\n",
    "    \"\"\"\n",
    "    # Parse and clean the input terms\n",
    "    terms = [term.strip() for term in term_list.split(',')]\n",
    "    terms = [term for term in terms if term]  # Remove empty strings\n",
    "    \n",
    "    print(f\"Checking {len(terms)} terms: {terms}\")\n",
    "    \n",
    "    # Convert to SQL-friendly format\n",
    "    terms_sql = \"','\".join(terms)\n",
    "    \n",
    "    # Query to find which terms exist in statements table\n",
    "    query = f\"\"\"\n",
    "    WITH input_terms AS (\n",
    "        SELECT explode(array('{terms_sql}')) as term\n",
    "    ),\n",
    "    found_terms AS (\n",
    "        SELECT DISTINCT\n",
    "            it.term as input_term,\n",
    "            s.subject as found_term,\n",
    "            s.predicate,\n",
    "            s.value as label\n",
    "        FROM input_terms it\n",
    "        LEFT JOIN {namespace}.statements s \n",
    "            ON it.term = s.subject\n",
    "        WHERE s.predicate = 'rdfs:label'\n",
    "    ),\n",
    "    all_found AS (\n",
    "        SELECT DISTINCT input_term as term\n",
    "        FROM found_terms\n",
    "        WHERE found_term IS NOT NULL\n",
    "    ),\n",
    "    missing AS (\n",
    "        SELECT term\n",
    "        FROM input_terms\n",
    "        WHERE term NOT IN (SELECT term FROM all_found)\n",
    "    )\n",
    "    SELECT \n",
    "        'found' as status,\n",
    "        f.input_term as term,\n",
    "        f.label as term_label\n",
    "    FROM found_terms f\n",
    "    WHERE f.found_term IS NOT NULL\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'missing' as status,\n",
    "        m.term,\n",
    "        NULL as term_label\n",
    "    FROM missing m\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute query\n",
    "    results_df = spark.sql(query).toPandas()\n",
    "    \n",
    "    # Separate found and missing terms\n",
    "    found_df = results_df[results_df['status'] == 'found'][['term', 'term_label']]\n",
    "    missing_df = results_df[results_df['status'] == 'missing'][['term']]\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nValidation Results:\")\n",
    "    print(f\"- Found: {len(found_df)} terms\")\n",
    "    print(f\"- Missing: {len(missing_df)} terms\")\n",
    "    \n",
    "    if len(missing_df) == 0:\n",
    "        print(\"\\n✓ All terms are part of the loaded ontology records!\")\n",
    "    else:\n",
    "        print(f\"\\n✗ {len(missing_df)} terms are NOT part of the loaded ontology records\")\n",
    "    \n",
    "    return found_df, missing_df\n",
    "\n",
    "# Example usage\n",
    "test_terms = \"OMP:0006023, ECO:0006056, OMP:9999999, ECO:0000001, FAKE:12345\"\n",
    "found_terms, missing_terms = validate_ontology_terms(test_terms)\n",
    "\n",
    "print(\"\\nFound terms:\")\n",
    "display(found_terms)\n",
    "\n",
    "if len(missing_terms) > 0:\n",
    "    print(\"\\nMissing terms:\")\n",
    "    display(missing_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Alternative Validation Method - Check Multiple Tables\n",
    "\n",
    "This method checks for term existence across multiple CDM tables for more comprehensive validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_terms_comprehensive(term_list: str) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Comprehensive validation checking multiple tables.\n",
    "    Returns detailed information about where each term was found.\n",
    "    \"\"\"\n",
    "    terms = [term.strip() for term in term_list.split(',')]\n",
    "    terms = [term for term in terms if term]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for term in terms:\n",
    "        # Check in statements table\n",
    "        statements_query = f\"\"\"\n",
    "        SELECT COUNT(*) as count\n",
    "        FROM {namespace}.statements\n",
    "        WHERE subject = '{term}' OR object = '{term}'\n",
    "        \"\"\"\n",
    "        statements_count = spark.sql(statements_query).collect()[0]['count']\n",
    "        \n",
    "        # Check in entailed_edge table\n",
    "        entailed_query = f\"\"\"\n",
    "        SELECT COUNT(*) as count\n",
    "        FROM {namespace}.entailed_edge\n",
    "        WHERE subject = '{term}' OR object = '{term}'\n",
    "        \"\"\"\n",
    "        entailed_count = spark.sql(entailed_query).collect()[0]['count']\n",
    "        \n",
    "        # Check in term_association table\n",
    "        term_assoc_query = f\"\"\"\n",
    "        SELECT COUNT(*) as count\n",
    "        FROM {namespace}.term_association\n",
    "        WHERE subject = '{term}' OR object = '{term}'\n",
    "        \"\"\"\n",
    "        term_assoc_count = spark.sql(term_assoc_query).collect()[0]['count']\n",
    "        \n",
    "        # Get label if exists\n",
    "        label_query = f\"\"\"\n",
    "        SELECT value as label\n",
    "        FROM {namespace}.statements\n",
    "        WHERE subject = '{term}' AND predicate = 'rdfs:label'\n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "        label_result = spark.sql(label_query).collect()\n",
    "        label = label_result[0]['label'] if label_result else None\n",
    "        \n",
    "        results[term] = {\n",
    "            'exists': (statements_count + entailed_count + term_assoc_count) > 0,\n",
    "            'label': label,\n",
    "            'in_statements': statements_count,\n",
    "            'in_entailed_edge': entailed_count,\n",
    "            'in_term_association': term_assoc_count,\n",
    "            'total_occurrences': statements_count + entailed_count + term_assoc_count\n",
    "        }\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for term, info in results.items():\n",
    "        summary_data.append({\n",
    "            'term': term,\n",
    "            'exists': info['exists'],\n",
    "            'label': info['label'],\n",
    "            'statements': info['in_statements'],\n",
    "            'entailed_edge': info['in_entailed_edge'],\n",
    "            'term_association': info['in_term_association'],\n",
    "            'total': info['total_occurrences']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Print summary\n",
    "    existing = summary_df[summary_df['exists']]\n",
    "    missing = summary_df[~summary_df['exists']]\n",
    "    \n",
    "    print(f\"\\nComprehensive Validation Results:\")\n",
    "    print(f\"- Found: {len(existing)} terms\")\n",
    "    print(f\"- Missing: {len(missing)} terms\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Test comprehensive validation\n",
    "comprehensive_results = validate_terms_comprehensive(test_terms)\n",
    "display(comprehensive_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Querying OMP (Ontology of Microbial Phenotypes) Terms\n",
    "\n",
    "Let's explore OMP terms and their relationships in the CDM ontology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count OMP terms\n",
    "def explore_omp_terms():\n",
    "    # Count total OMP terms\n",
    "    count_query = f\"\"\"\n",
    "    SELECT COUNT(DISTINCT subject) as omp_term_count\n",
    "    FROM {namespace}.statements\n",
    "    WHERE subject LIKE 'OMP:%'\n",
    "    \"\"\"\n",
    "    \n",
    "    count = spark.sql(count_query).collect()[0]['omp_term_count']\n",
    "    print(f\"Total OMP terms in the ontology: {count:,}\\n\")\n",
    "    \n",
    "    # Get sample OMP terms with labels\n",
    "    sample_query = f\"\"\"\n",
    "    SELECT \n",
    "        subject as omp_term,\n",
    "        value as label\n",
    "    FROM {namespace}.statements\n",
    "    WHERE subject LIKE 'OMP:%'\n",
    "    AND predicate = 'rdfs:label'\n",
    "    LIMIT 20\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_df = spark.sql(sample_query).toPandas()\n",
    "    print(\"Sample OMP terms:\")\n",
    "    display(sample_df)\n",
    "    \n",
    "    return sample_df\n",
    "\n",
    "omp_samples = explore_omp_terms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query OMP term hierarchy\n",
    "def get_omp_hierarchy(omp_term: str):\n",
    "    \"\"\"\n",
    "    Get the hierarchy (parents and children) for a specific OMP term.\n",
    "    \"\"\"\n",
    "    print(f\"Querying hierarchy for: {omp_term}\\n\")\n",
    "    \n",
    "    # Get term label\n",
    "    label_query = f\"\"\"\n",
    "    SELECT value as label\n",
    "    FROM {namespace}.statements\n",
    "    WHERE subject = '{omp_term}' AND predicate = 'rdfs:label'\n",
    "    \"\"\"\n",
    "    label_result = spark.sql(label_query).collect()\n",
    "    if label_result:\n",
    "        print(f\"Term: {omp_term}\")\n",
    "        print(f\"Label: {label_result[0]['label']}\\n\")\n",
    "    \n",
    "    # Get parents\n",
    "    parents_query = f\"\"\"\n",
    "    SELECT \n",
    "        s1.object as parent_term,\n",
    "        s2.value as parent_label\n",
    "    FROM {namespace}.statements s1\n",
    "    LEFT JOIN {namespace}.statements s2\n",
    "        ON s1.object = s2.subject AND s2.predicate = 'rdfs:label'\n",
    "    WHERE s1.subject = '{omp_term}'\n",
    "    AND s1.predicate = 'rdfs:subClassOf'\n",
    "    AND s1.object LIKE 'OMP:%'\n",
    "    \"\"\"\n",
    "    \n",
    "    parents_df = spark.sql(parents_query).toPandas()\n",
    "    print(f\"Parent terms ({len(parents_df)}):\")\n",
    "    display(parents_df)\n",
    "    \n",
    "    # Get children\n",
    "    children_query = f\"\"\"\n",
    "    SELECT \n",
    "        s1.subject as child_term,\n",
    "        s2.value as child_label\n",
    "    FROM {namespace}.statements s1\n",
    "    LEFT JOIN {namespace}.statements s2\n",
    "        ON s1.subject = s2.subject AND s2.predicate = 'rdfs:label'\n",
    "    WHERE s1.object = '{omp_term}'\n",
    "    AND s1.predicate = 'rdfs:subClassOf'\n",
    "    AND s1.subject LIKE 'OMP:%'\n",
    "    LIMIT 10\n",
    "    \"\"\"\n",
    "    \n",
    "    children_df = spark.sql(children_query).toPandas()\n",
    "    print(f\"\\nChild terms (showing up to 10 of possibly more):\")\n",
    "    display(children_df)\n",
    "    \n",
    "    return parents_df, children_df\n",
    "\n",
    "# Example: Get hierarchy for a phenotype term\n",
    "if len(omp_samples) > 0:\n",
    "    example_term = omp_samples.iloc[0]['omp_term']\n",
    "    parents, children = get_omp_hierarchy(example_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search OMP terms by keyword\n",
    "def search_omp_by_keyword(keyword: str):\n",
    "    \"\"\"\n",
    "    Search for OMP terms containing a specific keyword in their label.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        subject as omp_term,\n",
    "        value as label\n",
    "    FROM {namespace}.statements\n",
    "    WHERE subject LIKE 'OMP:%'\n",
    "    AND predicate = 'rdfs:label'\n",
    "    AND LOWER(value) LIKE LOWER('%{keyword}%')\n",
    "    ORDER BY value\n",
    "    LIMIT 20\n",
    "    \"\"\"\n",
    "    \n",
    "    results_df = spark.sql(query).toPandas()\n",
    "    print(f\"OMP terms containing '{keyword}' (showing up to 20):\")\n",
    "    display(results_df)\n",
    "    \n",
    "    # Count total matches\n",
    "    count_query = f\"\"\"\n",
    "    SELECT COUNT(*) as total_matches\n",
    "    FROM {namespace}.statements\n",
    "    WHERE subject LIKE 'OMP:%'\n",
    "    AND predicate = 'rdfs:label'\n",
    "    AND LOWER(value) LIKE LOWER('%{keyword}%')\n",
    "    \"\"\"\n",
    "    \n",
    "    total = spark.sql(count_query).collect()[0]['total_matches']\n",
    "    print(f\"\\nTotal OMP terms matching '{keyword}': {total}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Search examples\n",
    "growth_terms = search_omp_by_keyword('growth')\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "resistance_terms = search_omp_by_keyword('resistance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Querying ECO (Evidence and Conclusion Ontology) Terms\n",
    "\n",
    "Let's explore ECO terms which describe evidence types used in biological research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore ECO terms\n",
    "def explore_eco_terms():\n",
    "    # Count total ECO terms\n",
    "    count_query = f\"\"\"\n",
    "    SELECT COUNT(DISTINCT subject) as eco_term_count\n",
    "    FROM {namespace}.statements\n",
    "    WHERE subject LIKE 'ECO:%'\n",
    "    \"\"\"\n",
    "    \n",
    "    count = spark.sql(count_query).collect()[0]['eco_term_count']\n",
    "    print(f\"Total ECO terms in the ontology: {count:,}\\n\")\n",
    "    \n",
    "    # Get ECO term categories\n",
    "    categories_query = f\"\"\"\n",
    "    WITH eco_top_level AS (\n",
    "        SELECT DISTINCT\n",
    "            s1.subject as eco_term,\n",
    "            s1.value as label,\n",
    "            s2.object as parent\n",
    "        FROM {namespace}.statements s1\n",
    "        LEFT JOIN {namespace}.statements s2\n",
    "            ON s1.subject = s2.subject AND s2.predicate = 'rdfs:subClassOf'\n",
    "        WHERE s1.subject LIKE 'ECO:%'\n",
    "        AND s1.predicate = 'rdfs:label'\n",
    "    )\n",
    "    SELECT \n",
    "        eco_term,\n",
    "        label,\n",
    "        CASE \n",
    "            WHEN label LIKE '%experimental%' THEN 'Experimental evidence'\n",
    "            WHEN label LIKE '%computational%' THEN 'Computational evidence'\n",
    "            WHEN label LIKE '%similarity%' THEN 'Similarity evidence'\n",
    "            WHEN label LIKE '%manual%' THEN 'Manual assertion'\n",
    "            WHEN label LIKE '%automatic%' THEN 'Automatic assertion'\n",
    "            ELSE 'Other evidence type'\n",
    "        END as evidence_category\n",
    "    FROM eco_top_level\n",
    "    LIMIT 30\n",
    "    \"\"\"\n",
    "    \n",
    "    categories_df = spark.sql(categories_query).toPandas()\n",
    "    print(\"Sample ECO terms by category:\")\n",
    "    display(categories_df)\n",
    "    \n",
    "    return categories_df\n",
    "\n",
    "eco_samples = explore_eco_terms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ECO evidence types used in feature annotations\n",
    "def get_eco_usage_in_annotations():\n",
    "    \"\"\"\n",
    "    Find which ECO terms are actually used in feature annotations.\n",
    "    \"\"\"\n",
    "    # Check if ECO terms are used in term_association\n",
    "    usage_query = f\"\"\"\n",
    "    WITH eco_in_associations AS (\n",
    "        SELECT \n",
    "            ta.predicate,\n",
    "            ta.object as eco_term,\n",
    "            COUNT(*) as usage_count\n",
    "        FROM {namespace}.term_association ta\n",
    "        WHERE ta.object LIKE 'ECO:%'\n",
    "        GROUP BY ta.predicate, ta.object\n",
    "    ),\n",
    "    eco_with_labels AS (\n",
    "        SELECT \n",
    "            ea.predicate,\n",
    "            ea.eco_term,\n",
    "            ea.usage_count,\n",
    "            s.value as eco_label\n",
    "        FROM eco_in_associations ea\n",
    "        LEFT JOIN {namespace}.statements s\n",
    "            ON ea.eco_term = s.subject AND s.predicate = 'rdfs:label'\n",
    "    )\n",
    "    SELECT * FROM eco_with_labels\n",
    "    ORDER BY usage_count DESC\n",
    "    LIMIT 20\n",
    "    \"\"\"\n",
    "    \n",
    "    usage_df = spark.sql(usage_query).toPandas()\n",
    "    \n",
    "    if len(usage_df) > 0:\n",
    "        print(\"ECO terms used in term associations:\")\n",
    "        display(usage_df)\n",
    "    else:\n",
    "        print(\"No ECO terms found in term_association table.\")\n",
    "        \n",
    "        # Check other tables\n",
    "        print(\"\\nChecking for ECO terms in statements as evidence qualifiers...\")\n",
    "        \n",
    "        evidence_query = f\"\"\"\n",
    "        SELECT \n",
    "            predicate,\n",
    "            object as eco_term,\n",
    "            COUNT(*) as usage_count\n",
    "        FROM {namespace}.statements\n",
    "        WHERE object LIKE 'ECO:%'\n",
    "        GROUP BY predicate, object\n",
    "        ORDER BY usage_count DESC\n",
    "        LIMIT 10\n",
    "        \"\"\"\n",
    "        \n",
    "        evidence_df = spark.sql(evidence_query).toPandas()\n",
    "        if len(evidence_df) > 0:\n",
    "            print(\"ECO terms used as objects in statements:\")\n",
    "            display(evidence_df)\n",
    "    \n",
    "    return usage_df\n",
    "\n",
    "eco_usage = get_eco_usage_in_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search ECO terms by evidence type\n",
    "def search_eco_by_type(evidence_type: str):\n",
    "    \"\"\"\n",
    "    Search for ECO terms by evidence type (e.g., 'experimental', 'computational', 'manual')\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        subject as eco_term,\n",
    "        value as label\n",
    "    FROM {namespace}.statements\n",
    "    WHERE subject LIKE 'ECO:%'\n",
    "    AND predicate = 'rdfs:label'\n",
    "    AND LOWER(value) LIKE LOWER('%{evidence_type}%')\n",
    "    ORDER BY value\n",
    "    LIMIT 15\n",
    "    \"\"\"\n",
    "    \n",
    "    results_df = spark.sql(query).toPandas()\n",
    "    print(f\"ECO terms for '{evidence_type}' evidence:\")\n",
    "    display(results_df)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Examples of different evidence types\n",
    "experimental_eco = search_eco_by_type('experimental')\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "computational_eco = search_eco_by_type('computational')\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "similarity_eco = search_eco_by_type('similarity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Ontology Queries\n",
    "\n",
    "Let's look at relationships between different ontologies (if any exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cross_ontology_relationships():\n",
    "    \"\"\"\n",
    "    Find relationships between terms from different ontologies.\n",
    "    \"\"\"\n",
    "    # Look for statements that connect different ontology prefixes\n",
    "    query = f\"\"\"\n",
    "    WITH cross_refs AS (\n",
    "        SELECT \n",
    "            s.subject,\n",
    "            s.predicate,\n",
    "            s.object,\n",
    "            SUBSTRING_INDEX(s.subject, ':', 1) as subject_prefix,\n",
    "            SUBSTRING_INDEX(s.object, ':', 1) as object_prefix\n",
    "        FROM {namespace}.statements s\n",
    "        WHERE s.subject LIKE '%:%' \n",
    "        AND s.object LIKE '%:%'\n",
    "        AND SUBSTRING_INDEX(s.subject, ':', 1) != SUBSTRING_INDEX(s.object, ':', 1)\n",
    "    )\n",
    "    SELECT \n",
    "        subject_prefix,\n",
    "        object_prefix,\n",
    "        predicate,\n",
    "        COUNT(*) as relationship_count\n",
    "    FROM cross_refs\n",
    "    WHERE subject_prefix IN ('OMP', 'ECO', 'GO', 'seed', 'EC')\n",
    "    OR object_prefix IN ('OMP', 'ECO', 'GO', 'seed', 'EC')\n",
    "    GROUP BY subject_prefix, object_prefix, predicate\n",
    "    ORDER BY relationship_count DESC\n",
    "    LIMIT 20\n",
    "    \"\"\"\n",
    "    \n",
    "    cross_ref_df = spark.sql(query).toPandas()\n",
    "    print(\"Cross-ontology relationships:\")\n",
    "    display(cross_ref_df)\n",
    "    \n",
    "    # Show some examples\n",
    "    if len(cross_ref_df) > 0:\n",
    "        top_relationship = cross_ref_df.iloc[0]\n",
    "        example_query = f\"\"\"\n",
    "        SELECT \n",
    "            s.subject,\n",
    "            s.predicate,\n",
    "            s.object,\n",
    "            s1.value as subject_label,\n",
    "            s2.value as object_label\n",
    "        FROM {namespace}.statements s\n",
    "        LEFT JOIN {namespace}.statements s1\n",
    "            ON s.subject = s1.subject AND s1.predicate = 'rdfs:label'\n",
    "        LEFT JOIN {namespace}.statements s2\n",
    "            ON s.object = s2.subject AND s2.predicate = 'rdfs:label'\n",
    "        WHERE SUBSTRING_INDEX(s.subject, ':', 1) = '{top_relationship['subject_prefix']}'\n",
    "        AND SUBSTRING_INDEX(s.object, ':', 1) = '{top_relationship['object_prefix']}'\n",
    "        AND s.predicate = '{top_relationship['predicate']}'\n",
    "        LIMIT 5\n",
    "        \"\"\"\n",
    "        \n",
    "        examples_df = spark.sql(example_query).toPandas()\n",
    "        print(f\"\\nExample relationships between {top_relationship['subject_prefix']} and {top_relationship['object_prefix']}:\")\n",
    "        display(examples_df)\n",
    "    \n",
    "    return cross_ref_df\n",
    "\n",
    "cross_ontology_rels = find_cross_ontology_relationships()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Validation Function\n",
    "\n",
    "A convenient function for validating large batches of terms with detailed reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_validate_terms(terms_list: List[str], output_file: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Validate a large batch of ontology terms and optionally save results to a file.\n",
    "    \n",
    "    Args:\n",
    "        terms_list: List of ontology term strings\n",
    "        output_file: Optional CSV file path to save results\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with validation results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for term in terms_list:\n",
    "        term = term.strip()\n",
    "        \n",
    "        # Check existence and get label\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            '{term}' as term,\n",
    "            CASE WHEN COUNT(*) > 0 THEN true ELSE false END as exists,\n",
    "            MAX(CASE WHEN predicate = 'rdfs:label' THEN value END) as label,\n",
    "            COUNT(*) as total_statements,\n",
    "            COUNT(DISTINCT predicate) as predicate_count,\n",
    "            SUBSTRING_INDEX('{term}', ':', 1) as ontology_prefix\n",
    "        FROM {namespace}.statements\n",
    "        WHERE subject = '{term}'\n",
    "        \"\"\"\n",
    "        \n",
    "        result = spark.sql(query).collect()[0]\n",
    "        \n",
    "        results.append({\n",
    "            'term': term,\n",
    "            'exists': result['exists'],\n",
    "            'label': result['label'],\n",
    "            'ontology': result['ontology_prefix'],\n",
    "            'statement_count': result['total_statements'],\n",
    "            'property_count': result['predicate_count']\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Add summary statistics\n",
    "    total_terms = len(results_df)\n",
    "    found_terms = len(results_df[results_df['exists']])\n",
    "    missing_terms = total_terms - found_terms\n",
    "    \n",
    "    print(f\"\\nBatch Validation Summary:\")\n",
    "    print(f\"- Total terms checked: {total_terms}\")\n",
    "    print(f\"- Found: {found_terms} ({found_terms/total_terms*100:.1f}%)\")\n",
    "    print(f\"- Missing: {missing_terms} ({missing_terms/total_terms*100:.1f}%)\")\n",
    "    \n",
    "    # Group by ontology\n",
    "    ontology_summary = results_df.groupby('ontology').agg({\n",
    "        'term': 'count',\n",
    "        'exists': 'sum'\n",
    "    }).rename(columns={'term': 'total', 'exists': 'found'})\n",
    "    ontology_summary['missing'] = ontology_summary['total'] - ontology_summary['found']\n",
    "    \n",
    "    print(\"\\nBy Ontology:\")\n",
    "    display(ontology_summary)\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if output_file:\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nResults saved to: {output_file}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Example usage with multiple ontology terms\n",
    "test_batch = [\n",
    "    \"OMP:0006023\", \"OMP:0000144\", \"OMP:0007564\",\n",
    "    \"ECO:0006056\", \"ECO:0000001\", \"ECO:0000269\",\n",
    "    \"GO:0008150\", \"GO:0003674\", \"GO:0005575\",\n",
    "    \"FAKE:00001\", \"TEST:12345\"\n",
    "]\n",
    "\n",
    "batch_results = batch_validate_terms(test_batch)\n",
    "print(\"\\nDetailed Results:\")\n",
    "display(batch_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides several methods for:\n",
    "\n",
    "1. **Validating ontology terms**: Check if terms exist in the CDM ontology database\n",
    "2. **Comprehensive validation**: Check terms across multiple tables\n",
    "3. **OMP queries**: Explore microbial phenotype terms, hierarchies, and search by keywords\n",
    "4. **ECO queries**: Explore evidence ontology terms and their usage\n",
    "5. **Cross-ontology analysis**: Find relationships between different ontologies\n",
    "6. **Batch validation**: Process large lists of terms with detailed reporting\n",
    "\n",
    "The validation functions will return:\n",
    "- Lists of found vs. missing terms\n",
    "- Term labels and descriptions where available\n",
    "- Usage statistics across different tables\n",
    "- Summary messages indicating if all terms exist or which ones are missing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}