{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontology Term Validation and Querying\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Check if a list of ontology terms exists in the CDM ontology tables\n",
    "2. Query OMP (Ontology of Microbial Phenotypes) terms\n",
    "3. Query ECO (Evidence and Conclusion Ontology) terms\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/17 21:46:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/17 21:46:38 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/07/17 21:46:39 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to spark-job-logs/jplfaria/app-20250717214638-0000.inprogress. This is unsupported\n",
      "25/07/17 21:46:39 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from spark.utils import get_spark_session\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = get_spark_session()\n",
    "namespace = 'ontology_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Term Existence Validation\n",
    "\n",
    "This function takes a list of ontology terms and checks which ones exist in the CDM ontology tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def validate_ontology_terms(term_list: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Validate a list of ontology terms against the CDM ontology database.\n    \n    Args:\n        term_list: Comma-separated string of ontology terms (e.g., \"OMP:0006023, ECO:0006056\")\n    \n    Returns:\n        Tuple of (found_terms_df, missing_terms_df)\n    \"\"\"\n    # Parse and clean the input terms\n    terms = [term.strip() for term in term_list.split(',')]\n    terms = [term for term in terms if term]  # Remove empty strings\n    \n    print(f\"Checking {len(terms)} terms: {terms}\")\n    \n    # Convert to SQL-friendly format\n    terms_sql = \"','\".join(terms)\n    \n    # Query to find which terms exist in statements table\n    query = f\"\"\"\n    WITH input_terms AS (\n        SELECT explode(array('{terms_sql}')) as term\n    ),\n    found_terms AS (\n        SELECT DISTINCT\n            it.term as input_term,\n            s.subject as found_term,\n            s.predicate,\n            s.value as label\n        FROM input_terms it\n        LEFT JOIN {namespace}.statements s \n            ON it.term = s.subject\n        WHERE s.predicate = 'rdfs:label'\n    ),\n    all_found AS (\n        SELECT DISTINCT input_term as term\n        FROM found_terms\n        WHERE found_term IS NOT NULL\n    ),\n    missing AS (\n        SELECT term\n        FROM input_terms\n        WHERE term NOT IN (SELECT term FROM all_found)\n    )\n    SELECT \n        'found' as status,\n        f.input_term as term,\n        f.label as term_label\n    FROM found_terms f\n    WHERE f.found_term IS NOT NULL\n    \n    UNION ALL\n    \n    SELECT \n        'missing' as status,\n        m.term,\n        NULL as term_label\n    FROM missing m\n    \"\"\"\n    \n    # Execute query and collect results\n    results = spark.sql(query).collect()\n    \n    # Convert to pandas DataFrames manually\n    found_data = []\n    missing_data = []\n    \n    for row in results:\n        if row['status'] == 'found':\n            found_data.append({\n                'term': row['term'],\n                'term_label': row['term_label']\n            })\n        else:\n            missing_data.append({\n                'term': row['term']\n            })\n    \n    found_df = pd.DataFrame(found_data)\n    missing_df = pd.DataFrame(missing_data)\n    \n    # Print summary\n    print(f\"\\nValidation Results:\")\n    print(f\"- Found: {len(found_df)} terms\")\n    print(f\"- Missing: {len(missing_df)} terms\")\n    \n    if len(missing_df) == 0:\n        print(\"\\n✓ All terms are part of the loaded ontology records!\")\n    else:\n        print(f\"\\n✗ {len(missing_df)} terms are NOT part of the loaded ontology records\")\n    \n    return found_df, missing_df\n\n# Example usage\ntest_terms = \"OMP:0006023, ECO:0006056, OMP:9999999, ECO:0000001, FAKE:12345\"\nfound_terms, missing_terms = validate_ontology_terms(test_terms)\n\nprint(\"\\nFound terms:\")\ndisplay(found_terms)\n\nif len(missing_terms) > 0:\n    print(\"\\nMissing terms:\")\n    display(missing_terms)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Alternative Validation Method - Check Multiple Tables\n",
    "\n",
    "This method checks for term existence across multiple CDM tables for more comprehensive validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comprehensive Validation Results:\n",
      "- Found: 3 terms\n",
      "- Missing: 2 terms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>exists</th>\n",
       "      <th>label</th>\n",
       "      <th>statements</th>\n",
       "      <th>entailed_edge</th>\n",
       "      <th>term_association</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OMP:0006023</td>\n",
       "      <td>True</td>\n",
       "      <td>carbon source utilization phenotype</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ECO:0006056</td>\n",
       "      <td>True</td>\n",
       "      <td>high throughput evidence used in manual assertion</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OMP:9999999</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ECO:0000001</td>\n",
       "      <td>True</td>\n",
       "      <td>inference from background scientific knowledge</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FAKE:12345</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          term  exists                                              label  \\\n",
       "0  OMP:0006023    True                carbon source utilization phenotype   \n",
       "1  ECO:0006056    True  high throughput evidence used in manual assertion   \n",
       "2  OMP:9999999   False                                               None   \n",
       "3  ECO:0000001    True     inference from background scientific knowledge   \n",
       "4   FAKE:12345   False                                               None   \n",
       "\n",
       "   statements  entailed_edge  term_association  total  \n",
       "0          25             15                 0     40  \n",
       "1          24              5                 0     29  \n",
       "2           0              0                 0      0  \n",
       "3          11              5                 0     16  \n",
       "4           0              0                 0      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def validate_terms_comprehensive(term_list: str) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Comprehensive validation checking multiple tables.\n",
    "    Returns detailed information about where each term was found.\n",
    "    \"\"\"\n",
    "    terms = [term.strip() for term in term_list.split(',')]\n",
    "    terms = [term for term in terms if term]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for term in terms:\n",
    "        # Check in statements table\n",
    "        statements_query = f\"\"\"\n",
    "        SELECT COUNT(*) as count\n",
    "        FROM {namespace}.statements\n",
    "        WHERE subject = '{term}' OR object = '{term}'\n",
    "        \"\"\"\n",
    "        statements_count = spark.sql(statements_query).collect()[0]['count']\n",
    "        \n",
    "        # Check in entailed_edge table\n",
    "        entailed_query = f\"\"\"\n",
    "        SELECT COUNT(*) as count\n",
    "        FROM {namespace}.entailed_edge\n",
    "        WHERE subject = '{term}' OR object = '{term}'\n",
    "        \"\"\"\n",
    "        entailed_count = spark.sql(entailed_query).collect()[0]['count']\n",
    "        \n",
    "        # Check in term_association table\n",
    "        term_assoc_query = f\"\"\"\n",
    "        SELECT COUNT(*) as count\n",
    "        FROM {namespace}.term_association\n",
    "        WHERE subject = '{term}' OR object = '{term}'\n",
    "        \"\"\"\n",
    "        term_assoc_count = spark.sql(term_assoc_query).collect()[0]['count']\n",
    "        \n",
    "        # Get label if exists\n",
    "        label_query = f\"\"\"\n",
    "        SELECT value as label\n",
    "        FROM {namespace}.statements\n",
    "        WHERE subject = '{term}' AND predicate = 'rdfs:label'\n",
    "        LIMIT 1\n",
    "        \"\"\"\n",
    "        label_result = spark.sql(label_query).collect()\n",
    "        label = label_result[0]['label'] if label_result else None\n",
    "        \n",
    "        results[term] = {\n",
    "            'exists': (statements_count + entailed_count + term_assoc_count) > 0,\n",
    "            'label': label,\n",
    "            'in_statements': statements_count,\n",
    "            'in_entailed_edge': entailed_count,\n",
    "            'in_term_association': term_assoc_count,\n",
    "            'total_occurrences': statements_count + entailed_count + term_assoc_count\n",
    "        }\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for term, info in results.items():\n",
    "        summary_data.append({\n",
    "            'term': term,\n",
    "            'exists': info['exists'],\n",
    "            'label': info['label'],\n",
    "            'statements': info['in_statements'],\n",
    "            'entailed_edge': info['in_entailed_edge'],\n",
    "            'term_association': info['in_term_association'],\n",
    "            'total': info['total_occurrences']\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Print summary\n",
    "    existing = summary_df[summary_df['exists']]\n",
    "    missing = summary_df[~summary_df['exists']]\n",
    "    \n",
    "    print(f\"\\nComprehensive Validation Results:\")\n",
    "    print(f\"- Found: {len(existing)} terms\")\n",
    "    print(f\"- Missing: {len(missing)} terms\")\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "# Test comprehensive validation\n",
    "comprehensive_results = validate_terms_comprehensive(test_terms)\n",
    "display(comprehensive_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Querying OMP (Ontology of Microbial Phenotypes) Terms\n",
    "\n",
    "Let's explore OMP terms and their relationships in the CDM ontology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Count OMP terms\ndef explore_omp_terms():\n    # Count total OMP terms\n    count_query = f\"\"\"\n    SELECT COUNT(DISTINCT subject) as omp_term_count\n    FROM {namespace}.statements\n    WHERE subject LIKE 'OMP:%'\n    \"\"\"\n    \n    count = spark.sql(count_query).collect()[0]['omp_term_count']\n    print(f\"Total OMP terms in the ontology: {count:,}\\n\")\n    \n    # Get sample OMP terms with labels\n    sample_query = f\"\"\"\n    SELECT \n        subject as omp_term,\n        value as label\n    FROM {namespace}.statements\n    WHERE subject LIKE 'OMP:%'\n    AND predicate = 'rdfs:label'\n    LIMIT 20\n    \"\"\"\n    \n    # Collect results and convert to DataFrame manually\n    results = spark.sql(sample_query).collect()\n    sample_data = []\n    for row in results:\n        sample_data.append({\n            'omp_term': row['omp_term'],\n            'label': row['label']\n        })\n    \n    sample_df = pd.DataFrame(sample_data)\n    print(\"Sample OMP terms:\")\n    display(sample_df)\n    \n    return sample_df\n\nomp_samples = explore_omp_terms()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Query OMP term hierarchy\ndef get_omp_hierarchy(omp_term: str):\n    \"\"\"\n    Get the hierarchy (parents and children) for a specific OMP term.\n    \"\"\"\n    print(f\"Querying hierarchy for: {omp_term}\\n\")\n    \n    # Get term label\n    label_query = f\"\"\"\n    SELECT value as label\n    FROM {namespace}.statements\n    WHERE subject = '{omp_term}' AND predicate = 'rdfs:label'\n    \"\"\"\n    label_result = spark.sql(label_query).collect()\n    if label_result:\n        print(f\"Term: {omp_term}\")\n        print(f\"Label: {label_result[0]['label']}\\n\")\n    \n    # Get parents\n    parents_query = f\"\"\"\n    SELECT \n        s1.object as parent_term,\n        s2.value as parent_label\n    FROM {namespace}.statements s1\n    LEFT JOIN {namespace}.statements s2\n        ON s1.object = s2.subject AND s2.predicate = 'rdfs:label'\n    WHERE s1.subject = '{omp_term}'\n    AND s1.predicate = 'rdfs:subClassOf'\n    AND s1.object LIKE 'OMP:%'\n    \"\"\"\n    \n    # Collect results and convert to DataFrame manually\n    parent_results = spark.sql(parents_query).collect()\n    parent_data = []\n    for row in parent_results:\n        parent_data.append({\n            'parent_term': row['parent_term'],\n            'parent_label': row['parent_label']\n        })\n    \n    parents_df = pd.DataFrame(parent_data)\n    print(f\"Parent terms ({len(parents_df)}):\")\n    display(parents_df)\n    \n    # Get children\n    children_query = f\"\"\"\n    SELECT \n        s1.subject as child_term,\n        s2.value as child_label\n    FROM {namespace}.statements s1\n    LEFT JOIN {namespace}.statements s2\n        ON s1.subject = s2.subject AND s2.predicate = 'rdfs:label'\n    WHERE s1.object = '{omp_term}'\n    AND s1.predicate = 'rdfs:subClassOf'\n    AND s1.subject LIKE 'OMP:%'\n    LIMIT 10\n    \"\"\"\n    \n    # Collect results and convert to DataFrame manually\n    child_results = spark.sql(children_query).collect()\n    child_data = []\n    for row in child_results:\n        child_data.append({\n            'child_term': row['child_term'],\n            'child_label': row['child_label']\n        })\n    \n    children_df = pd.DataFrame(child_data)\n    print(f\"\\nChild terms (showing up to 10 of possibly more):\")\n    display(children_df)\n    \n    return parents_df, children_df\n\n# Example: Get hierarchy for a phenotype term\nif len(omp_samples) > 0:\n    example_term = omp_samples.iloc[0]['omp_term']\n    parents, children = get_omp_hierarchy(example_term)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Search OMP terms by keyword\ndef search_omp_by_keyword(keyword: str):\n    \"\"\"\n    Search for OMP terms containing a specific keyword in their label.\n    \"\"\"\n    query = f\"\"\"\n    SELECT \n        subject as omp_term,\n        value as label\n    FROM {namespace}.statements\n    WHERE subject LIKE 'OMP:%'\n    AND predicate = 'rdfs:label'\n    AND LOWER(value) LIKE LOWER('%{keyword}%')\n    ORDER BY value\n    LIMIT 20\n    \"\"\"\n    \n    # Collect results and convert to DataFrame manually\n    results = spark.sql(query).collect()\n    result_data = []\n    for row in results:\n        result_data.append({\n            'omp_term': row['omp_term'],\n            'label': row['label']\n        })\n    \n    results_df = pd.DataFrame(result_data)\n    print(f\"OMP terms containing '{keyword}' (showing up to 20):\")\n    display(results_df)\n    \n    # Count total matches\n    count_query = f\"\"\"\n    SELECT COUNT(*) as total_matches\n    FROM {namespace}.statements\n    WHERE subject LIKE 'OMP:%'\n    AND predicate = 'rdfs:label'\n    AND LOWER(value) LIKE LOWER('%{keyword}%')\n    \"\"\"\n    \n    total = spark.sql(count_query).collect()[0]['total_matches']\n    print(f\"\\nTotal OMP terms matching '{keyword}': {total}\")\n    \n    return results_df\n\n# Search examples\ngrowth_terms = search_omp_by_keyword('growth')\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nresistance_terms = search_omp_by_keyword('resistance')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Querying ECO (Evidence and Conclusion Ontology) Terms\n",
    "\n",
    "Let's explore ECO terms which describe evidence types used in biological research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Explore ECO terms\ndef explore_eco_terms():\n    # Count total ECO terms\n    count_query = f\"\"\"\n    SELECT COUNT(DISTINCT subject) as eco_term_count\n    FROM {namespace}.statements\n    WHERE subject LIKE 'ECO:%'\n    \"\"\"\n    \n    count = spark.sql(count_query).collect()[0]['eco_term_count']\n    print(f\"Total ECO terms in the ontology: {count:,}\\n\")\n    \n    # Get ECO term categories\n    categories_query = f\"\"\"\n    WITH eco_top_level AS (\n        SELECT DISTINCT\n            s1.subject as eco_term,\n            s1.value as label,\n            s2.object as parent\n        FROM {namespace}.statements s1\n        LEFT JOIN {namespace}.statements s2\n            ON s1.subject = s2.subject AND s2.predicate = 'rdfs:subClassOf'\n        WHERE s1.subject LIKE 'ECO:%'\n        AND s1.predicate = 'rdfs:label'\n    )\n    SELECT \n        eco_term,\n        label,\n        CASE \n            WHEN label LIKE '%experimental%' THEN 'Experimental evidence'\n            WHEN label LIKE '%computational%' THEN 'Computational evidence'\n            WHEN label LIKE '%similarity%' THEN 'Similarity evidence'\n            WHEN label LIKE '%manual%' THEN 'Manual assertion'\n            WHEN label LIKE '%automatic%' THEN 'Automatic assertion'\n            ELSE 'Other evidence type'\n        END as evidence_category\n    FROM eco_top_level\n    LIMIT 30\n    \"\"\"\n    \n    # Collect results and convert to DataFrame manually\n    results = spark.sql(categories_query).collect()\n    category_data = []\n    for row in results:\n        category_data.append({\n            'eco_term': row['eco_term'],\n            'label': row['label'],\n            'evidence_category': row['evidence_category']\n        })\n    \n    categories_df = pd.DataFrame(category_data)\n    print(\"Sample ECO terms by category:\")\n    display(categories_df)\n    \n    return categories_df\n\neco_samples = explore_eco_terms()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get ECO evidence types used in feature annotations\ndef get_eco_usage_in_annotations():\n    \"\"\"\n    Find which ECO terms are actually used in feature annotations.\n    \"\"\"\n    # Check if ECO terms are used in term_association\n    usage_query = f\"\"\"\n    WITH eco_in_associations AS (\n        SELECT \n            ta.predicate,\n            ta.object as eco_term,\n            COUNT(*) as usage_count\n        FROM {namespace}.term_association ta\n        WHERE ta.object LIKE 'ECO:%'\n        GROUP BY ta.predicate, ta.object\n    ),\n    eco_with_labels AS (\n        SELECT \n            ea.predicate,\n            ea.eco_term,\n            ea.usage_count,\n            s.value as eco_label\n        FROM eco_in_associations ea\n        LEFT JOIN {namespace}.statements s\n            ON ea.eco_term = s.subject AND s.predicate = 'rdfs:label'\n    )\n    SELECT * FROM eco_with_labels\n    ORDER BY usage_count DESC\n    LIMIT 20\n    \"\"\"\n    \n    # Collect results and convert to DataFrame manually\n    results = spark.sql(usage_query).collect()\n    usage_data = []\n    for row in results:\n        usage_data.append({\n            'predicate': row['predicate'],\n            'eco_term': row['eco_term'],\n            'usage_count': row['usage_count'],\n            'eco_label': row['eco_label']\n        })\n    \n    usage_df = pd.DataFrame(usage_data)\n    \n    if len(usage_df) > 0:\n        print(\"ECO terms used in term associations:\")\n        display(usage_df)\n    else:\n        print(\"No ECO terms found in term_association table.\")\n        \n        # Check other tables\n        print(\"\\nChecking for ECO terms in statements as evidence qualifiers...\")\n        \n        evidence_query = f\"\"\"\n        SELECT \n            predicate,\n            object as eco_term,\n            COUNT(*) as usage_count\n        FROM {namespace}.statements\n        WHERE object LIKE 'ECO:%'\n        GROUP BY predicate, object\n        ORDER BY usage_count DESC\n        LIMIT 10\n        \"\"\"\n        \n        # Collect results and convert to DataFrame manually\n        evidence_results = spark.sql(evidence_query).collect()\n        evidence_data = []\n        for row in evidence_results:\n            evidence_data.append({\n                'predicate': row['predicate'],\n                'eco_term': row['eco_term'],\n                'usage_count': row['usage_count']\n            })\n        \n        evidence_df = pd.DataFrame(evidence_data)\n        if len(evidence_df) > 0:\n            print(\"ECO terms used as objects in statements:\")\n            display(evidence_df)\n    \n    return usage_df\n\neco_usage = get_eco_usage_in_annotations()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Search ECO terms by evidence type\ndef search_eco_by_type(evidence_type: str):\n    \"\"\"\n    Search for ECO terms by evidence type (e.g., 'experimental', 'computational', 'manual')\n    \"\"\"\n    query = f\"\"\"\n    SELECT \n        subject as eco_term,\n        value as label\n    FROM {namespace}.statements\n    WHERE subject LIKE 'ECO:%'\n    AND predicate = 'rdfs:label'\n    AND LOWER(value) LIKE LOWER('%{evidence_type}%')\n    ORDER BY value\n    LIMIT 15\n    \"\"\"\n    \n    # Collect results and convert to DataFrame manually\n    results = spark.sql(query).collect()\n    result_data = []\n    for row in results:\n        result_data.append({\n            'eco_term': row['eco_term'],\n            'label': row['label']\n        })\n    \n    results_df = pd.DataFrame(result_data)\n    print(f\"ECO terms for '{evidence_type}' evidence:\")\n    display(results_df)\n    \n    return results_df\n\n# Examples of different evidence types\nexperimental_eco = search_eco_by_type('experimental')\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\ncomputational_eco = search_eco_by_type('computational')\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nsimilarity_eco = search_eco_by_type('similarity')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cross-Ontology Queries\n",
    "\n",
    "Let's look at relationships between different ontologies (if any exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def find_cross_ontology_relationships():\n    \"\"\"\n    Find relationships between terms from different ontologies.\n    \"\"\"\n    # Look for statements that connect different ontology prefixes\n    query = f\"\"\"\n    WITH cross_refs AS (\n        SELECT \n            s.subject,\n            s.predicate,\n            s.object,\n            SUBSTRING_INDEX(s.subject, ':', 1) as subject_prefix,\n            SUBSTRING_INDEX(s.object, ':', 1) as object_prefix\n        FROM {namespace}.statements s\n        WHERE s.subject LIKE '%:%' \n        AND s.object LIKE '%:%'\n        AND SUBSTRING_INDEX(s.subject, ':', 1) != SUBSTRING_INDEX(s.object, ':', 1)\n    )\n    SELECT \n        subject_prefix,\n        object_prefix,\n        predicate,\n        COUNT(*) as relationship_count\n    FROM cross_refs\n    WHERE subject_prefix IN ('OMP', 'ECO', 'GO', 'seed', 'EC')\n    OR object_prefix IN ('OMP', 'ECO', 'GO', 'seed', 'EC')\n    GROUP BY subject_prefix, object_prefix, predicate\n    ORDER BY relationship_count DESC\n    LIMIT 20\n    \"\"\"\n    \n    # Collect results and convert to DataFrame manually\n    results = spark.sql(query).collect()\n    cross_ref_data = []\n    for row in results:\n        cross_ref_data.append({\n            'subject_prefix': row['subject_prefix'],\n            'object_prefix': row['object_prefix'],\n            'predicate': row['predicate'],\n            'relationship_count': row['relationship_count']\n        })\n    \n    cross_ref_df = pd.DataFrame(cross_ref_data)\n    print(\"Cross-ontology relationships:\")\n    display(cross_ref_df)\n    \n    # Show some examples\n    if len(cross_ref_df) > 0:\n        top_relationship = cross_ref_df.iloc[0]\n        example_query = f\"\"\"\n        SELECT \n            s.subject,\n            s.predicate,\n            s.object,\n            s1.value as subject_label,\n            s2.value as object_label\n        FROM {namespace}.statements s\n        LEFT JOIN {namespace}.statements s1\n            ON s.subject = s1.subject AND s1.predicate = 'rdfs:label'\n        LEFT JOIN {namespace}.statements s2\n            ON s.object = s2.subject AND s2.predicate = 'rdfs:label'\n        WHERE SUBSTRING_INDEX(s.subject, ':', 1) = '{top_relationship['subject_prefix']}'\n        AND SUBSTRING_INDEX(s.object, ':', 1) = '{top_relationship['object_prefix']}'\n        AND s.predicate = '{top_relationship['predicate']}'\n        LIMIT 5\n        \"\"\"\n        \n        # Collect example results and convert to DataFrame manually\n        example_results = spark.sql(example_query).collect()\n        example_data = []\n        for row in example_results:\n            example_data.append({\n                'subject': row['subject'],\n                'predicate': row['predicate'],\n                'object': row['object'],\n                'subject_label': row['subject_label'],\n                'object_label': row['object_label']\n            })\n        \n        examples_df = pd.DataFrame(example_data)\n        print(f\"\\nExample relationships between {top_relationship['subject_prefix']} and {top_relationship['object_prefix']}:\")\n        display(examples_df)\n    \n    return cross_ref_df\n\ncross_ontology_rels = find_cross_ontology_relationships()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Batch Validation Function\n",
    "\n",
    "A convenient function for validating large batches of terms with detailed reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 285:============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch Validation Summary:\n",
      "- Total terms checked: 11\n",
      "- Found: 9 (81.8%)\n",
      "- Missing: 2 (18.2%)\n",
      "\n",
      "By Ontology:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>found</th>\n",
       "      <th>missing</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ontology</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ECO</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FAKE</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OMP</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TEST</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          total  found  missing\n",
       "ontology                       \n",
       "ECO           3      3        0\n",
       "FAKE          1      0        1\n",
       "GO            3      3        0\n",
       "OMP           3      3        0\n",
       "TEST          1      0        1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>exists</th>\n",
       "      <th>label</th>\n",
       "      <th>ontology</th>\n",
       "      <th>statement_count</th>\n",
       "      <th>property_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OMP:0006023</td>\n",
       "      <td>True</td>\n",
       "      <td>carbon source utilization phenotype</td>\n",
       "      <td>OMP</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OMP:0000144</td>\n",
       "      <td>True</td>\n",
       "      <td>multiple nucleoids</td>\n",
       "      <td>OMP</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OMP:0007564</td>\n",
       "      <td>True</td>\n",
       "      <td>altered meiotic nuclear division</td>\n",
       "      <td>OMP</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ECO:0006056</td>\n",
       "      <td>True</td>\n",
       "      <td>high throughput evidence used in manual assertion</td>\n",
       "      <td>ECO</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ECO:0000001</td>\n",
       "      <td>True</td>\n",
       "      <td>inference from background scientific knowledge</td>\n",
       "      <td>ECO</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ECO:0000269</td>\n",
       "      <td>True</td>\n",
       "      <td>experimental evidence used in manual assertion</td>\n",
       "      <td>ECO</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GO:0008150</td>\n",
       "      <td>True</td>\n",
       "      <td>biological_process</td>\n",
       "      <td>GO</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GO:0003674</td>\n",
       "      <td>True</td>\n",
       "      <td>molecular_function</td>\n",
       "      <td>GO</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GO:0005575</td>\n",
       "      <td>True</td>\n",
       "      <td>cellular_component</td>\n",
       "      <td>GO</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FAKE:00001</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>TEST:12345</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>TEST</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           term  exists                                              label  \\\n",
       "0   OMP:0006023    True                carbon source utilization phenotype   \n",
       "1   OMP:0000144    True                                 multiple nucleoids   \n",
       "2   OMP:0007564    True                   altered meiotic nuclear division   \n",
       "3   ECO:0006056    True  high throughput evidence used in manual assertion   \n",
       "4   ECO:0000001    True     inference from background scientific knowledge   \n",
       "5   ECO:0000269    True     experimental evidence used in manual assertion   \n",
       "6    GO:0008150    True                                 biological_process   \n",
       "7    GO:0003674    True                                 molecular_function   \n",
       "8    GO:0005575    True                                 cellular_component   \n",
       "9    FAKE:00001   False                                               None   \n",
       "10   TEST:12345   False                                               None   \n",
       "\n",
       "   ontology  statement_count  property_count  \n",
       "0       OMP               18              12  \n",
       "1       OMP                9               9  \n",
       "2       OMP               10               9  \n",
       "3       ECO               19              15  \n",
       "4       ECO                7               7  \n",
       "5       ECO               16              14  \n",
       "6        GO               25              15  \n",
       "7        GO               15              10  \n",
       "8        GO               19              13  \n",
       "9      FAKE                0               0  \n",
       "10     TEST                0               0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def batch_validate_terms(terms_list: List[str], output_file: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Validate a large batch of ontology terms and optionally save results to a file.\n",
    "    \n",
    "    Args:\n",
    "        terms_list: List of ontology term strings\n",
    "        output_file: Optional CSV file path to save results\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with validation results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for term in terms_list:\n",
    "        term = term.strip()\n",
    "        \n",
    "        # Check existence and get label\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            '{term}' as term,\n",
    "            CASE WHEN COUNT(*) > 0 THEN true ELSE false END as exists,\n",
    "            MAX(CASE WHEN predicate = 'rdfs:label' THEN value END) as label,\n",
    "            COUNT(*) as total_statements,\n",
    "            COUNT(DISTINCT predicate) as predicate_count,\n",
    "            SUBSTRING_INDEX('{term}', ':', 1) as ontology_prefix\n",
    "        FROM {namespace}.statements\n",
    "        WHERE subject = '{term}'\n",
    "        \"\"\"\n",
    "        \n",
    "        result = spark.sql(query).collect()[0]\n",
    "        \n",
    "        results.append({\n",
    "            'term': term,\n",
    "            'exists': result['exists'],\n",
    "            'label': result['label'],\n",
    "            'ontology': result['ontology_prefix'],\n",
    "            'statement_count': result['total_statements'],\n",
    "            'property_count': result['predicate_count']\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Add summary statistics\n",
    "    total_terms = len(results_df)\n",
    "    found_terms = len(results_df[results_df['exists']])\n",
    "    missing_terms = total_terms - found_terms\n",
    "    \n",
    "    print(f\"\\nBatch Validation Summary:\")\n",
    "    print(f\"- Total terms checked: {total_terms}\")\n",
    "    print(f\"- Found: {found_terms} ({found_terms/total_terms*100:.1f}%)\")\n",
    "    print(f\"- Missing: {missing_terms} ({missing_terms/total_terms*100:.1f}%)\")\n",
    "    \n",
    "    # Group by ontology\n",
    "    ontology_summary = results_df.groupby('ontology').agg({\n",
    "        'term': 'count',\n",
    "        'exists': 'sum'\n",
    "    }).rename(columns={'term': 'total', 'exists': 'found'})\n",
    "    ontology_summary['missing'] = ontology_summary['total'] - ontology_summary['found']\n",
    "    \n",
    "    print(\"\\nBy Ontology:\")\n",
    "    display(ontology_summary)\n",
    "    \n",
    "    # Save to file if requested\n",
    "    if output_file:\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nResults saved to: {output_file}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Example usage with multiple ontology terms\n",
    "test_batch = [\n",
    "    \"OMP:0006023\", \"OMP:0000144\", \"OMP:0007564\",\n",
    "    \"ECO:0006056\", \"ECO:0000001\", \"ECO:0000269\",\n",
    "    \"GO:0008150\", \"GO:0003674\", \"GO:0005575\",\n",
    "    \"FAKE:00001\", \"TEST:12345\"\n",
    "]\n",
    "\n",
    "batch_results = batch_validate_terms(test_batch)\n",
    "print(\"\\nDetailed Results:\")\n",
    "display(batch_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides several methods for:\n",
    "\n",
    "1. **Validating ontology terms**: Check if terms exist in the CDM ontology database\n",
    "2. **Comprehensive validation**: Check terms across multiple tables\n",
    "3. **OMP queries**: Explore microbial phenotype terms, hierarchies, and search by keywords\n",
    "4. **ECO queries**: Explore evidence ontology terms and their usage\n",
    "5. **Cross-ontology analysis**: Find relationships between different ontologies\n",
    "6. **Batch validation**: Process large lists of terms with detailed reporting\n",
    "\n",
    "The validation functions will return:\n",
    "- Lists of found vs. missing terms\n",
    "- Term labels and descriptions where available\n",
    "- Usage statistics across different tables\n",
    "- Summary messages indicating if all terms exist or which ones are missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}