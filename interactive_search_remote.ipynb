{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Data Search Tool (Remote Version)\n",
    "\n",
    "This notebook provides an interactive way to search through the CDM ontology data tables using Spark.\n",
    "\n",
    "**Note**: This version is designed to work on the remote JupyterHub with Spark tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark.utils import get_spark_session\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create Spark session\n",
    "spark = get_spark_session()\n",
    "namespace = 'ontology_data'\n",
    "\n",
    "print(\"Spark session created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Available Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables in the namespace\n",
    "tables_df = spark.sql(f\"SHOW TABLES IN {namespace}\")\n",
    "tables = [row.tableName for row in tables_df.collect()]\n",
    "\n",
    "print(f\"Available tables in {namespace}:\")\n",
    "for table in tables:\n",
    "    count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {namespace}.{table}\").collect()[0]['cnt']\n",
    "    print(f\"  - {table}: {count:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Search Functions\n",
    "\n",
    "These functions allow searching through Spark tables efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_table(table_name, search_term, column=None, limit=100):\n",
    "    \"\"\"Search for a term in a specific table\"\"\"\n",
    "    \n",
    "    if column:\n",
    "        # Search specific column\n",
    "        query = f\"\"\"\n",
    "        SELECT * \n",
    "        FROM {namespace}.{table_name}\n",
    "        WHERE LOWER({column}) LIKE LOWER('%{search_term}%')\n",
    "        LIMIT {limit}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        # Get columns for this table\n",
    "        cols = spark.sql(f\"SELECT * FROM {namespace}.{table_name} LIMIT 1\").columns\n",
    "        \n",
    "        # Build WHERE clause for all string columns\n",
    "        where_clauses = []\n",
    "        for col in cols:\n",
    "            where_clauses.append(f\"LOWER(CAST({col} AS STRING)) LIKE LOWER('%{search_term}%')\")\n",
    "        \n",
    "        where_condition = \" OR \".join(where_clauses)\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT * \n",
    "        FROM {namespace}.{table_name}\n",
    "        WHERE {where_condition}\n",
    "        LIMIT {limit}\n",
    "        \"\"\"\n",
    "    \n",
    "    try:\n",
    "        results_df = spark.sql(query)\n",
    "        pandas_df = results_df.toPandas()\n",
    "        return pandas_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error searching {table_name}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def search_all_tables(search_term, limit_per_table=50):\n",
    "    \"\"\"Search across all tables\"\"\"\n",
    "    all_results = {}\n",
    "    \n",
    "    for table in tables:\n",
    "        if table == 'prefix':  # Skip prefix table as requested\n",
    "            continue\n",
    "            \n",
    "        print(f\"Searching {table}...\")\n",
    "        results = search_table(table, search_term, limit=limit_per_table)\n",
    "        \n",
    "        if not results.empty:\n",
    "            all_results[table] = results\n",
    "            print(f\"  Found {len(results)} matches\")\n",
    "    \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for a specific term\n",
    "search_term = \"glucose\"  # Change this to search for different terms\n",
    "\n",
    "print(f\"Searching for '{search_term}' across all tables...\\n\")\n",
    "results = search_all_tables(search_term, limit_per_table=20)\n",
    "\n",
    "# Display results\n",
    "for table, df in results.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Results from {table}: {len(df)} matches\")\n",
    "    print(f\"{'='*60}\")\n",
    "    display(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specialized Search Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_seed_compounds(compound_pattern=None, limit=100):\n",
    "    \"\"\"Find SEED compounds with their details\"\"\"\n",
    "    \n",
    "    if compound_pattern:\n",
    "        pattern_filter = f\"AND s1.subject LIKE '%{compound_pattern}%'\"\n",
    "    else:\n",
    "        pattern_filter = \"\"\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        s1.subject as compound_id,\n",
    "        s1.value as compound_name,\n",
    "        s2.object as cross_reference,\n",
    "        s2.predicate as ref_type\n",
    "    FROM {namespace}.statements s1\n",
    "    LEFT JOIN {namespace}.statements s2 \n",
    "        ON s1.subject = s2.subject \n",
    "        AND s2.predicate IN ('oio:hasDbXref', 'skos:exactMatch')\n",
    "    WHERE s1.subject LIKE 'seed.compound:%'\n",
    "    AND s1.predicate = 'rdfs:label'\n",
    "    {pattern_filter}\n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "    \n",
    "    return spark.sql(query).toPandas()\n",
    "\n",
    "# Example: Find glucose-related compounds\n",
    "glucose_compounds = find_seed_compounds('glucose')\n",
    "print(f\"Found {len(glucose_compounds)} SEED compounds related to glucose:\")\n",
    "display(glucose_compounds.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ec_numbers_with_details(ec_pattern=None, limit=100):\n",
    "    \"\"\"Find EC numbers with their names and associated reactions\"\"\"\n",
    "    \n",
    "    ec_filter = f\"WHERE f.bakta_ec LIKE '{ec_pattern}%'\" if ec_pattern else \"\"\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH ec_in_genomes AS (\n",
    "        SELECT DISTINCT\n",
    "            bakta_ec as ec_number,\n",
    "            COUNT(DISTINCT genome_id) as genome_count,\n",
    "            COUNT(*) as feature_count,\n",
    "            FIRST(bakta_product) as example_product\n",
    "        FROM {namespace}.feature_annotation\n",
    "        WHERE bakta_ec IS NOT NULL\n",
    "        GROUP BY bakta_ec\n",
    "    ),\n",
    "    ec_details AS (\n",
    "        SELECT \n",
    "            subject as ec_id,\n",
    "            value as ec_name\n",
    "        FROM {namespace}.statements\n",
    "        WHERE predicate = 'rdfs:label'\n",
    "        AND subject LIKE 'EC:%'\n",
    "    )\n",
    "    SELECT \n",
    "        e.ec_number,\n",
    "        d.ec_name,\n",
    "        e.genome_count,\n",
    "        e.feature_count,\n",
    "        e.example_product\n",
    "    FROM ec_in_genomes e\n",
    "    LEFT JOIN ec_details d ON CONCAT('EC:', e.ec_number) = d.ec_id\n",
    "    {ec_filter.replace('f.', 'e.')}\n",
    "    ORDER BY e.genome_count DESC, e.feature_count DESC\n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "    \n",
    "    return spark.sql(query).toPandas()\n",
    "\n",
    "# Example: Find all transferases (EC 2.*)\n",
    "transferases = find_ec_numbers_with_details('2')\n",
    "print(f\"Found {len(transferases)} transferase EC numbers:\")\n",
    "display(transferases.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_go_terms(go_pattern=None, limit=100):\n",
    "    \"\"\"Find GO terms with their usage in genomes\"\"\"\n",
    "    \n",
    "    go_filter = f\"WHERE f.bakta_go LIKE '%{go_pattern}%'\" if go_pattern else \"\"\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH go_usage AS (\n",
    "        SELECT \n",
    "            bakta_go as go_term,\n",
    "            COUNT(DISTINCT genome_id) as genome_count,\n",
    "            COUNT(*) as annotation_count,\n",
    "            COLLECT_SET(bakta_product)[0] as example_product\n",
    "        FROM {namespace}.feature_annotation f\n",
    "        WHERE bakta_go IS NOT NULL\n",
    "        {go_filter}\n",
    "        GROUP BY bakta_go\n",
    "    ),\n",
    "    go_details AS (\n",
    "        SELECT \n",
    "            subject as go_id,\n",
    "            value as go_name\n",
    "        FROM {namespace}.statements\n",
    "        WHERE predicate = 'rdfs:label'\n",
    "        AND subject LIKE 'GO:%'\n",
    "    )\n",
    "    SELECT \n",
    "        g.go_term,\n",
    "        d.go_name,\n",
    "        g.genome_count,\n",
    "        g.annotation_count,\n",
    "        g.example_product,\n",
    "        ROUND(g.genome_count * 100.0 / 50, 2) as pct_genomes\n",
    "    FROM go_usage g\n",
    "    LEFT JOIN go_details d ON g.go_term = d.go_id\n",
    "    ORDER BY g.genome_count DESC, g.annotation_count DESC\n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "    \n",
    "    return spark.sql(query).toPandas()\n",
    "\n",
    "# Example: Find binding-related GO terms\n",
    "binding_terms = find_go_terms('binding')\n",
    "print(f\"Found {len(binding_terms)} GO terms related to binding:\")\n",
    "display(binding_terms.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_search_to_csv(search_term, output_path='search_results'):\n",
    "    \"\"\"Export search results to CSV files\"\"\"\n",
    "    \n",
    "    results = search_all_tables(search_term, limit_per_table=1000)\n",
    "    \n",
    "    for table, df in results.items():\n",
    "        filename = f\"{output_path}_{table}_{search_term.replace(' ', '_')}.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Exported {len(df)} results from {table} to {filename}\")\n",
    "\n",
    "# Example: Export all glucose-related entries\n",
    "# export_search_to_csv('glucose', 'glucose_search')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Search: Find Metabolic Pathways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_metabolic_pathway(compound_name, limit=50):\n",
    "    \"\"\"Find reactions and enzymes related to a specific compound\"\"\"\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    WITH compound_reactions AS (\n",
    "        -- Find reactions involving the compound\n",
    "        SELECT DISTINCT\n",
    "            e.subject as reaction_id,\n",
    "            e.object as compound_id,\n",
    "            s.value as compound_name\n",
    "        FROM {namespace}.entailed_edge e\n",
    "        JOIN {namespace}.statements s\n",
    "            ON e.object = s.subject \n",
    "            AND s.predicate = 'rdfs:label'\n",
    "        WHERE e.predicate = 'RO:0000057'  -- has participant\n",
    "        AND e.subject LIKE 'seed.reaction:%'\n",
    "        AND LOWER(s.value) LIKE LOWER('%{compound_name}%')\n",
    "    ),\n",
    "    reaction_enzymes AS (\n",
    "        -- Find enzymes that catalyze these reactions\n",
    "        SELECT \n",
    "            ta.object as reaction_id,\n",
    "            ta.subject as role_id,\n",
    "            s.value as role_name\n",
    "        FROM {namespace}.term_association ta\n",
    "        JOIN {namespace}.statements s\n",
    "            ON ta.subject = s.subject\n",
    "            AND s.predicate = 'rdfs:label'\n",
    "        WHERE ta.predicate = 'RO:0002327'  -- enables\n",
    "    )\n",
    "    SELECT DISTINCT\n",
    "        cr.reaction_id,\n",
    "        cr.compound_name,\n",
    "        re.role_name as enzyme_role,\n",
    "        rs.value as reaction_name\n",
    "    FROM compound_reactions cr\n",
    "    LEFT JOIN reaction_enzymes re ON cr.reaction_id = re.reaction_id\n",
    "    LEFT JOIN {namespace}.statements rs \n",
    "        ON cr.reaction_id = rs.subject \n",
    "        AND rs.predicate = 'rdfs:label'\n",
    "    LIMIT {limit}\n",
    "    \"\"\"\n",
    "    \n",
    "    return spark.sql(query).toPandas()\n",
    "\n",
    "# Example: Find pathways involving pyruvate\n",
    "pyruvate_pathways = find_metabolic_pathway('pyruvate', limit=30)\n",
    "print(f\"Found {len(pyruvate_pathways)} reactions involving pyruvate:\")\n",
    "display(pyruvate_pathways)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics for all tables\n",
    "def get_table_statistics():\n",
    "    \"\"\"Get detailed statistics for each table\"\"\"\n",
    "    \n",
    "    for table in tables:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Table: {namespace}.{table}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Get row count\n",
    "        count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {namespace}.{table}\").collect()[0]['cnt']\n",
    "        print(f\"Total rows: {count:,}\")\n",
    "        \n",
    "        # Get schema\n",
    "        schema = spark.sql(f\"SELECT * FROM {namespace}.{table} LIMIT 1\").schema\n",
    "        print(f\"\\nColumns ({len(schema.fields)}):\")\n",
    "        for field in schema.fields:\n",
    "            print(f\"  - {field.name}: {field.dataType}\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(f\"\\nSample data:\")\n",
    "        sample_df = spark.sql(f\"SELECT * FROM {namespace}.{table} LIMIT 3\").toPandas()\n",
    "        display(sample_df)\n",
    "\n",
    "# Run statistics\n",
    "get_table_statistics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}