{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931eaea9-53b3-469e-923c-81b15e3305a8",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b508bec0-f798-4e28-9d34-2c1133d810c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from minio import Minio\n",
    "from pyspark import SparkContext, HiveContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from delta.tables import DeltaTable\n",
    "from spark.utils import get_spark_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f630660-d3fa-4ef7-b114-c280547ff026",
   "metadata": {},
   "source": [
    "### Get filenames from Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "816dd171-51e5-4c58-9878-8da5b38aa9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSV files   : []\n",
      "Parquet files: ['ontology-source/entailed_edge.parquet', 'ontology-source/feature_annotation.parquet', 'ontology-source/prefix.parquet', 'ontology-source/statements.parquet', 'ontology-source/term_association.parquet']\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------\n",
    "# Discover TSV and Parquet files in MinIO\n",
    "# -------------------------------------\n",
    "minio_client = get_minio_client()\n",
    "\n",
    "bucket         = \"cdm-lake\"\n",
    "source_prefix  = \"ontology-source/\"          # keep the trailing slash\n",
    "\n",
    "# → pull the generator into a list so we can iterate more than once\n",
    "objects = list(\n",
    "    minio_client.list_objects(bucket, prefix=source_prefix, recursive=True)\n",
    ")\n",
    "\n",
    "tsv_files = [obj.object_name for obj in objects if obj.object_name.endswith(\".tsv\")]\n",
    "parquet_files = [obj.object_name for obj in objects if obj.object_name.endswith(\".parquet\")]\n",
    "\n",
    "print(\"TSV files   :\", tsv_files)\n",
    "print(\"Parquet files:\", parquet_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5819859a-1960-46d6-b81c-0da1e20d19b9",
   "metadata": {},
   "source": [
    "### Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d687979d-d676-4b13-b4ea-e77e16feee0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/08 06:03:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/08 06:03:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/07/08 06:03:19 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/07/08 06:03:21 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to spark-job-logs/jplfaria/app-20250708060319-0012.inprogress. This is unsupported\n",
      "25/07/08 06:03:21 WARN Utils: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "spark = get_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee35f4c6-9f12-4a26-8a52-e7776bd46bda",
   "metadata": {},
   "source": [
    "### Create database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b49d231-81ad-4ec3-8105-8cf6884d0ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace ontology_data is ready to use.\n"
     ]
    }
   ],
   "source": [
    "namespace = 'ontology_data'\n",
    "create_namespace_if_not_exists(spark, namespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59042854-3b6b-4733-90dc-4343a9c970af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SHOW TABLES in {namespace}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abbb28a-b0e3-4e0c-aad3-1e669a738799",
   "metadata": {},
   "source": [
    "### DROP TABLE CELL BE CAREFUL- DONT RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8aca3ad-9b86-473f-aedf-9b03c5c4a39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped table: ontology_data.entailed_edge\n",
      "Dropped table: ontology_data.prefix\n",
      "Dropped table: ontology_data.statements\n",
      "Dropped table: ontology_data.term_association\n"
     ]
    }
   ],
   "source": [
    "# Get the list of tables\n",
    "tables = spark.sql(f\"SHOW TABLES IN {namespace}\").collect()\n",
    "\n",
    "# Drop each table\n",
    "for row in tables:\n",
    "    table_name = row.tableName\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {namespace}.{table_name}\")\n",
    "    print(f\"Dropped table: {namespace}.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f70d6da1-f0c6-4a61-b14a-353a75153430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SHOW TABLES in {namespace}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18349d2-53e0-47bd-ada5-ba695dbcc4f5",
   "metadata": {},
   "source": [
    "### Define and execute Delta table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014fd1a9-776f-4edd-8c1c-d5b8c626181f",
   "metadata": {},
   "source": [
    "#### Load parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f9298da-7e54-43a1-8b93-ef05d61fb0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing entailed_edge...\n",
      "Reading from s3a://cdm-lake/ontology-source/entailed_edge.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to s3a://cdm-lake/ontology-deltalake/entailed_edge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔  entailed_edge: 117545336 rows\n",
      "Processing feature_annotation...\n",
      "Reading from s3a://cdm-lake/ontology-source/feature_annotation.parquet\n",
      "Writing to s3a://cdm-lake/ontology-deltalake/feature_annotation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔  feature_annotation: 236843 rows\n",
      "Processing prefix...\n",
      "Reading from s3a://cdm-lake/ontology-source/prefix.parquet\n",
      "Writing to s3a://cdm-lake/ontology-deltalake/prefix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔  prefix: 1221 rows\n",
      "Processing statements...\n",
      "Reading from s3a://cdm-lake/ontology-source/statements.parquet\n",
      "Writing to s3a://cdm-lake/ontology-deltalake/statements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔  statements: 42373349 rows\n",
      "Processing term_association...\n",
      "Reading from s3a://cdm-lake/ontology-source/term_association.parquet\n",
      "Writing to s3a://cdm-lake/ontology-deltalake/term_association\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔  term_association: 3271 rows\n",
      "\n",
      "Summary:\n",
      "• entailed_edge: 117545336 rows\n",
      "• feature_annotation: 236843 rows\n",
      "• prefix: 1221 rows\n",
      "• statements: 42373349 rows\n",
      "• term_association: 3271 rows\n"
     ]
    }
   ],
   "source": [
    "def create_delta_table(spark, file_name, bucket=\"cdm-lake\"):\n",
    "    \"\"\"\n",
    "    Load a *Parquet* file from MinIO and materialise it as a Delta table.\n",
    "\n",
    "    Only the lines that changed compared with the TSV version are marked\n",
    "    with  # <-- changed\n",
    "    \"\"\"\n",
    "    # ---------- table / path handling ----------------------------------------------------------\n",
    "    table_name = os.path.splitext(os.path.basename(file_name))[0]        \n",
    "\n",
    "    print(f\"Processing {table_name}...\")\n",
    "    full_path = f\"s3a://{bucket}/{file_name}\"\n",
    "    print(f\"Reading from {full_path}\")\n",
    "\n",
    "    # ---------- READ (now Parquet) --------------------------------------------------------------\n",
    "    df = (spark.read\n",
    "                .format(\"parquet\")                                       \n",
    "                .load(full_path))                                        \n",
    "\n",
    "    # ---------- WRITE as Delta & register -------------------------------------------------------\n",
    "    delta_path = f\"s3a://{bucket}/ontology-deltalake/{table_name}\"\n",
    "    print(f\"Writing to {delta_path}\")\n",
    "\n",
    "    (df.write\n",
    "       .mode(\"overwrite\")\n",
    "       .option(\"compression\", \"snappy\")\n",
    "       .format(\"delta\")\n",
    "       .saveAsTable(f\"{namespace}.{table_name}\"))\n",
    "\n",
    "    return table_name, df.count()\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Example driver loop (now over a list of .parquet files)           |\n",
    "# ------------------------------------------------------------------\n",
    "results = []\n",
    "for parquet_file in parquet_files:      \n",
    "    try:\n",
    "        table, rows = create_delta_table(spark, parquet_file)\n",
    "        results.append({\"table\": table, \"rows\": rows})\n",
    "        print(f\"✔  {table}: {rows} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"✖  Error processing {parquet_file}: {e}\")\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "for r in results:\n",
    "    print(f\"• {r['table']}: {r['rows']} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fc352f-1f6a-47bc-bd8d-5252ba1e453b",
   "metadata": {},
   "source": [
    "#### Load as .tsv (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0851e1b-58ab-4988-b810-5f5e910b53d9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing entailed_edge...\n",
      "Reading from s3a://cdm-lake/ontology-source/entailed_edge.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to s3a://cdm-lake/ontology-deltalake/entailed_edge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed entailed_edge with 101472614 rows\n",
      "Processing prefix...\n",
      "Reading from s3a://cdm-lake/ontology-source/prefix.tsv\n",
      "Writing to s3a://cdm-lake/ontology-deltalake/prefix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed prefix with 1216 rows\n",
      "Processing statements...\n",
      "Reading from s3a://cdm-lake/ontology-source/statements.tsv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to s3a://cdm-lake/ontology-deltalake/statements\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:=====================================================>  (20 + 1) / 21]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed statements with 37960705 rows\n",
      "\n",
      "Processing Summary:\n",
      "Table entailed_edge: 101472614 rows\n",
      "Table prefix: 1216 rows\n",
      "Table statements: 37960705 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "def create_delta_table(spark, file_name, bucket=\"cdm-lake\"):\n",
    "    # Extract table name from file path\n",
    "    table_name = os.path.basename(file_name).replace('.tsv', '')\n",
    "    \n",
    "    print(f\"Processing {table_name}...\")\n",
    "    \n",
    "    # Read TSV file from MinIO\n",
    "    print(f\"Reading from s3a://{bucket}/{file_name}\")\n",
    "    df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"delimiter\", \"\\t\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .load(f\"s3a://{bucket}/{file_name}\")\n",
    "    \n",
    "    # Write as Delta table in MinIO\n",
    "    delta_path = f\"s3a://{bucket}/ontology-deltalake/{table_name}\"\n",
    "    print(f\"Writing to {delta_path}\")\n",
    "    \n",
    "#    df.write \\\n",
    "#        .format(\"delta\") \\\n",
    "#        .mode(\"overwrite\") \\\n",
    "#        .option(\"overwriteSchema\", \"true\") \\\n",
    "#        .save(delta_path)\n",
    "    \n",
    "    # Create table in the database\n",
    "#    spark.sql(f\"\"\"\n",
    "#    CREATE TABLE IF NOT EXISTS {namespace}.{table_name}\n",
    "#    USING DELTA\n",
    "#    LOCATION '{delta_path}'\n",
    "#    \"\"\")\n",
    "\n",
    "    df.write.mode(\n",
    "        \"overwrite\"\n",
    "        ).option(\"compression\", \"snappy\"\n",
    "        ).format(\"delta\"\n",
    "        ).saveAsTable(f\"{namespace}.{table_name}\"\n",
    "    )\n",
    "    \n",
    "    return table_name, df.count()\n",
    "\n",
    "# Process each TSV file\n",
    "results = []\n",
    "for tsv_file in tsv_files:\n",
    "    try:\n",
    "        table_name, row_count = create_delta_table(spark, tsv_file)\n",
    "        results.append({\"table\": table_name, \"rows\": row_count})\n",
    "        print(f\"Successfully processed {table_name} with {row_count} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {tsv_file}: {str(e)}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nProcessing Summary:\")\n",
    "for result in results:\n",
    "    print(f\"Table {result['table']}: {result['rows']} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd4fe43-c311-4298-855c-aacb9fd1bac5",
   "metadata": {},
   "source": [
    "### Verify the tables were created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1cebd44-329f-49bf-9877-a50ef38a2873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tables in database:\n",
      "+-------------+------------------+-----------+\n",
      "|    namespace|         tableName|isTemporary|\n",
      "+-------------+------------------+-----------+\n",
      "|ontology_data|     entailed_edge|      false|\n",
      "|ontology_data|feature_annotation|      false|\n",
      "|ontology_data|            prefix|      false|\n",
      "|ontology_data|        statements|      false|\n",
      "|ontology_data|  term_association|      false|\n",
      "+-------------+------------------+-----------+\n",
      "\n",
      "\n",
      "Sample data from statements:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------------+--------------+------------------------------------------------+--------+--------+-----+\n",
      "|stanza     |subject    |predicate       |object        |value                                           |datatype|language|graph|\n",
      "+-----------+-----------+----------------+--------------+------------------------------------------------+--------+--------+-----+\n",
      "|EC:2.8.3.23|EC:2.8.3.23|rdfs:subClassOf |EC:2.8.3      |NULL                                            |NULL    |NULL    |NULL |\n",
      "|EC:2.8.3.23|EC:2.8.3.23|rdf:type        |owl:Class     |NULL                                            |NULL    |NULL    |NULL |\n",
      "|EC:2.8.3.24|EC:2.8.3.24|rdfs:label      |NULL          |(R)-2-hydroxy-4-methylpentanoate CoA-transferase|NULL    |NULL    |NULL |\n",
      "|EC:2.8.3.24|EC:2.8.3.24|rdfs:isDefinedBy|obo:eccode.owl|NULL                                            |NULL    |NULL    |NULL |\n",
      "|EC:2.8.3.24|EC:2.8.3.24|oio:id          |NULL          |eccode:2.8.3.24                                 |NULL    |NULL    |NULL |\n",
      "+-----------+-----------+----------------+--------------+------------------------------------------------+--------+--------+-----+\n",
      "\n",
      "\n",
      "Sample data from entailed_edge:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------+----------------------+\n",
      "|subject               |predicate      |object                |\n",
      "+----------------------+---------------+----------------------+\n",
      "|NCBITaxon:250646      |rdfs:subClassOf|NCBITaxon:250646      |\n",
      "|NCBITaxon:2762532     |rdfs:subClassOf|NCBITaxon:2762532     |\n",
      "|seed.compound:cpd25778|rdfs:subClassOf|seed.compound:cpd25778|\n",
      "|NCBITaxon:912255      |rdfs:subClassOf|NCBITaxon:912255      |\n",
      "|UniProtKB:Q62MS6      |rdfs:subClassOf|UniProtKB:Q62MS6      |\n",
      "+----------------------+---------------+----------------------+\n",
      "\n",
      "\n",
      "Sample data from prefix:\n",
      "+------+-------------------------------------------+\n",
      "|prefix|base                                       |\n",
      "+------+-------------------------------------------+\n",
      "|prefix|base                                       |\n",
      "|rdf   |http://www.w3.org/1999/02/22-rdf-syntax-ns#|\n",
      "|rdfs  |http://www.w3.org/2000/01/rdf-schema#      |\n",
      "|xsd   |http://www.w3.org/2001/XMLSchema#          |\n",
      "|owl   |http://www.w3.org/2002/07/owl#             |\n",
      "+------+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List all tables\n",
    "print(\"Available tables in database:\")\n",
    "spark.sql(f\"SHOW TABLES IN {namespace}\").show()\n",
    "\n",
    "# Sample query to verify data\n",
    "for table in ['statements', 'entailed_edge', 'prefix']:\n",
    "    print(f\"\\nSample data from {table}:\")\n",
    "    spark.sql(f\"SELECT * FROM {namespace}.{table} LIMIT 5\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f4dfa4-b652-4f83-9986-8144e5931b62",
   "metadata": {},
   "source": [
    "### Verify MinIO structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b480508e-8fe9-4099-9200-104a41b04276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_minio_structure():\n",
    "    # ----------------------------\n",
    "    # 1)  show all source files\n",
    "    # ----------------------------\n",
    "    print(\"Source files:\")\n",
    "    for obj in minio_client.list_objects(bucket,\n",
    "                                         prefix=\"ontology-source/\",\n",
    "                                         recursive=True):        # <- True\n",
    "        if obj.object_name.endswith((\".tsv\", \".parquet\")):        # <- both\n",
    "            print(f\"  {os.path.basename(obj.object_name)}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2)  show Delta-table folders\n",
    "    # ----------------------------\n",
    "    print(\"\\nDelta tables (warehouse):\")\n",
    "    delta_prefix = \"warehouse/ontology_data.db/\"                  # <- new root\n",
    "    current_table = None\n",
    "\n",
    "    for obj in minio_client.list_objects(bucket,\n",
    "                                         prefix=delta_prefix,\n",
    "                                         recursive=True):\n",
    "        parts = obj.object_name[len(delta_prefix):].split(\"/\", 1) # strip prefix\n",
    "        table = parts[0]\n",
    "\n",
    "        if table != current_table:                                # new table dir\n",
    "            current_table = table\n",
    "            print(f\"\\n  {table}/\")\n",
    "\n",
    "        if len(parts) > 1:                                        # show leaf path\n",
    "            print(f\"    ├── {parts[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de5d908e-8f21-438b-a718-84b7acff58f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source files:\n",
      "  entailed_edge.parquet\n",
      "  feature_annotation.parquet\n",
      "  prefix.parquet\n",
      "  statements.parquet\n",
      "  term_association.parquet\n",
      "\n",
      "Delta tables (warehouse):\n",
      "\n",
      "  entailed_edge/\n",
      "    ├── _delta_log/00000000000000000000.json\n",
      "    ├── _delta_log/_commits/\n",
      "    ├── part-00000-5ecf2959-1831-402b-9473-7961c899f7f0-c000.snappy.parquet\n",
      "    ├── part-00001-767e8d37-a182-4cc4-a224-f41a50d2c812-c000.snappy.parquet\n",
      "    ├── part-00002-ec7a1702-48ad-4a71-92f7-4124c8c835b0-c000.snappy.parquet\n",
      "\n",
      "  feature_annotation/\n",
      "    ├── _delta_log/00000000000000000000.json\n",
      "    ├── _delta_log/_commits/\n",
      "    ├── part-00000-061dc51d-3e96-499e-8395-2a8261f9e82b-c000.snappy.parquet\n",
      "\n",
      "  prefix/\n",
      "    ├── _delta_log/00000000000000000000.json\n",
      "    ├── _delta_log/_commits/\n",
      "    ├── part-00000-c4ecc9ad-31c7-4d02-a41b-64726a765343-c000.snappy.parquet\n",
      "\n",
      "  statements/\n",
      "    ├── _delta_log/00000000000000000000.json\n",
      "    ├── _delta_log/_commits/\n",
      "    ├── part-00000-71b8bd1b-59ef-4625-b71a-1413d0bb1cfe-c000.snappy.parquet\n",
      "    ├── part-00001-88521d5f-341e-49ba-9d57-43d858f175c7-c000.snappy.parquet\n",
      "    ├── part-00002-62741d8c-c2aa-44f6-8233-5747b415063e-c000.snappy.parquet\n",
      "\n",
      "  term_association/\n",
      "    ├── _delta_log/00000000000000000000.json\n",
      "    ├── _delta_log/_commits/\n",
      "    ├── part-00000-38052e5c-5d7d-4278-b9ac-f0ddc109dfb7-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "verify_minio_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ae5150-349b-447f-9e61-18546c25630f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
