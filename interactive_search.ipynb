{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Data Search Tool\n",
    "\n",
    "This notebook provides an interactive way to search through the CDM ontology data tables.\n",
    "\n",
    "**Note**: This uses local parquet files for quick searching. Run on remote JupyterHub for full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from IPython.display import display, HTML\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For remote execution, uncomment these:\n",
    "# from spark.utils import get_spark_session\n",
    "# spark = get_spark_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Tables\n",
    "\n",
    "Loading smaller tables into memory for interactive searching. For the large tables, we'll use sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data files\n",
    "data_files = {\n",
    "    'statements': 'data/statements.parquet',\n",
    "    'entailed_edge': 'data/entailed_edge.parquet',\n",
    "    'feature_annotation': 'data/feature_annotation.parquet',\n",
    "    'term_association': 'data/term_association.parquet'\n",
    "    # Ignoring prefix.parquet as requested\n",
    "}\n",
    "\n",
    "# Load smaller tables completely, sample larger ones\n",
    "dataframes = {}\n",
    "table_info = {}\n",
    "\n",
    "for name, path in data_files.items():\n",
    "    print(f\"Loading {name}...\")\n",
    "    \n",
    "    # Get file info\n",
    "    parquet_file = pq.ParquetFile(path)\n",
    "    total_rows = parquet_file.metadata.num_rows\n",
    "    \n",
    "    if total_rows > 1_000_000:  # Sample large files\n",
    "        # Read only first 100k rows for interactive search\n",
    "        df = pd.read_parquet(path, engine='pyarrow').head(100_000)\n",
    "        table_info[name] = f\"Sampled: 100,000 of {total_rows:,} rows\"\n",
    "    else:\n",
    "        df = pd.read_parquet(path, engine='pyarrow')\n",
    "        table_info[name] = f\"Complete: {total_rows:,} rows\"\n",
    "    \n",
    "    dataframes[name] = df\n",
    "    print(f\"  Loaded {table_info[name]}\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Search Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create search interface\n",
    "def search_data(table, search_term, column='all', max_results=50):\n",
    "    \"\"\"Search for a term in the specified table and column\"\"\"\n",
    "    if not search_term:\n",
    "        return \"Enter a search term\"\n",
    "    \n",
    "    df = dataframes[table]\n",
    "    \n",
    "    # Convert search term to lowercase for case-insensitive search\n",
    "    search_term = search_term.lower()\n",
    "    \n",
    "    if column == 'all':\n",
    "        # Search across all string columns\n",
    "        mask = pd.Series([False] * len(df))\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                mask |= df[col].astype(str).str.lower().str.contains(search_term, na=False)\n",
    "    else:\n",
    "        # Search specific column\n",
    "        mask = df[column].astype(str).str.lower().str.contains(search_term, na=False)\n",
    "    \n",
    "    results = df[mask].head(max_results)\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        return f\"No results found for '{search_term}' in {table}\"\n",
    "    \n",
    "    # Display results with highlighting\n",
    "    html = f\"<h3>Found {len(df[mask])} results (showing first {len(results)})</h3>\"\n",
    "    html += f\"<p><em>{table_info[table]}</em></p>\"\n",
    "    \n",
    "    # Convert to HTML and highlight search term\n",
    "    results_html = results.to_html()\n",
    "    # Simple highlighting (case-insensitive)\n",
    "    import re\n",
    "    pattern = re.compile(re.escape(search_term), re.IGNORECASE)\n",
    "    results_html = pattern.sub(lambda m: f'<mark style=\"background-color: yellow\">{m.group()}</mark>', results_html)\n",
    "    \n",
    "    return HTML(html + results_html)\n",
    "\n",
    "# Create interactive widgets\n",
    "table_widget = widgets.Dropdown(\n",
    "    options=list(dataframes.keys()),\n",
    "    value='statements',\n",
    "    description='Table:'\n",
    ")\n",
    "\n",
    "search_widget = widgets.Text(\n",
    "    placeholder='Enter search term...',\n",
    "    description='Search:'\n",
    ")\n",
    "\n",
    "column_widget = widgets.Dropdown(\n",
    "    options=['all'],\n",
    "    value='all',\n",
    "    description='Column:'\n",
    ")\n",
    "\n",
    "max_results_widget = widgets.IntSlider(\n",
    "    value=50,\n",
    "    min=10,\n",
    "    max=200,\n",
    "    step=10,\n",
    "    description='Max Results:'\n",
    ")\n",
    "\n",
    "# Update column options when table changes\n",
    "def update_columns(*args):\n",
    "    column_widget.options = ['all'] + list(dataframes[table_widget.value].columns)\n",
    "    \n",
    "table_widget.observe(update_columns, 'value')\n",
    "update_columns()  # Initialize\n",
    "\n",
    "# Create search interface\n",
    "out = widgets.interactive_output(\n",
    "    search_data, \n",
    "    {\n",
    "        'table': table_widget, \n",
    "        'search_term': search_widget,\n",
    "        'column': column_widget,\n",
    "        'max_results': max_results_widget\n",
    "    }\n",
    ")\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([table_widget, column_widget]),\n",
    "    search_widget,\n",
    "    max_results_widget,\n",
    "    out\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Search Functions\n",
    "\n",
    "Pre-built searches for common queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_seed_entries(compound_id=None, reaction_id=None, role_id=None):\n",
    "    \"\"\"Find SEED database entries\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Search patterns\n",
    "    patterns = []\n",
    "    if compound_id:\n",
    "        patterns.append(f\"seed.compound:{compound_id}\")\n",
    "    if reaction_id:\n",
    "        patterns.append(f\"seed.reaction:{reaction_id}\")\n",
    "    if role_id:\n",
    "        patterns.append(f\"seed.role:{role_id}\")\n",
    "    \n",
    "    if not patterns:\n",
    "        patterns = ['seed.compound', 'seed.reaction', 'seed.role']\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        for table_name, df in dataframes.items():\n",
    "            mask = pd.Series([False] * len(df))\n",
    "            for col in df.columns:\n",
    "                if df[col].dtype == 'object':\n",
    "                    mask |= df[col].astype(str).str.contains(pattern, na=False, case=False)\n",
    "            \n",
    "            found = df[mask]\n",
    "            if len(found) > 0:\n",
    "                if table_name not in results:\n",
    "                    results[table_name] = pd.DataFrame()\n",
    "                results[table_name] = pd.concat([results[table_name], found]).drop_duplicates()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Find all seed.compound entries\n",
    "print(\"Searching for SEED compounds...\")\n",
    "seed_results = find_seed_entries(compound_id='cpd')\n",
    "for table, df in seed_results.items():\n",
    "    if len(df) > 0:\n",
    "        print(f\"\\n{table}: {len(df)} entries\")\n",
    "        display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ec_numbers(ec_pattern=None):\n",
    "    \"\"\"Find Enzyme Commission (EC) numbers\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # EC number pattern (e.g., EC:1.1.1.1)\n",
    "    if ec_pattern:\n",
    "        pattern = f\"EC:{ec_pattern}\"\n",
    "    else:\n",
    "        pattern = \"EC:\"\n",
    "    \n",
    "    for table_name, df in dataframes.items():\n",
    "        mask = pd.Series([False] * len(df))\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                mask |= df[col].astype(str).str.contains(pattern, na=False)\n",
    "        \n",
    "        found = df[mask]\n",
    "        if len(found) > 0:\n",
    "            results[table_name] = found\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Find specific EC class\n",
    "print(\"Searching for EC numbers starting with 2.8...\")\n",
    "ec_results = find_ec_numbers(\"2.8\")\n",
    "for table, df in ec_results.items():\n",
    "    print(f\"\\n{table}: {len(df)} entries\")\n",
    "    display(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ncbi_taxa(taxon_id=None, search_term=None):\n",
    "    \"\"\"Find NCBI Taxonomy entries\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    if taxon_id:\n",
    "        pattern = f\"NCBITaxon:{taxon_id}\"\n",
    "    elif search_term:\n",
    "        pattern = search_term\n",
    "    else:\n",
    "        pattern = \"NCBITaxon:\"\n",
    "    \n",
    "    for table_name, df in dataframes.items():\n",
    "        mask = pd.Series([False] * len(df))\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                mask |= df[col].astype(str).str.contains(pattern, na=False, case=False)\n",
    "        \n",
    "        found = df[mask]\n",
    "        if len(found) > 0:\n",
    "            results[table_name] = found\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "print(\"Searching for NCBI Taxonomy entries...\")\n",
    "taxa_results = find_ncbi_taxa()\n",
    "for table, df in taxa_results.items():\n",
    "    print(f\"\\n{table}: {len(df)} entries with NCBI Taxon IDs\")\n",
    "    # Show unique subjects if applicable\n",
    "    if 'subject' in df.columns:\n",
    "        unique_taxa = df['subject'].str.extract(r'(NCBITaxon:\\d+)')[0].dropna().unique()\n",
    "        print(f\"  Unique taxa: {len(unique_taxa)}\")\n",
    "        print(f\"  Examples: {list(unique_taxa[:5])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Statistics Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary dashboard\n",
    "def create_data_summary():\n",
    "    \"\"\"Create a summary of all data tables\"\"\"\n",
    "    summary_html = \"<h2>Data Tables Summary</h2>\"\n",
    "    \n",
    "    for name, df in dataframes.items():\n",
    "        summary_html += f\"<h3>{name}</h3>\"\n",
    "        summary_html += f\"<p><em>{table_info[name]}</em></p>\"\n",
    "        summary_html += \"<ul>\"\n",
    "        summary_html += f\"<li>Columns: {', '.join(df.columns)}</li>\"\n",
    "        \n",
    "        # Column statistics\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                unique_vals = df[col].nunique()\n",
    "                summary_html += f\"<li>{col}: {unique_vals:,} unique values</li>\"\n",
    "        \n",
    "        summary_html += \"</ul>\"\n",
    "        \n",
    "        # Show sample\n",
    "        summary_html += \"<details><summary>View sample data</summary>\"\n",
    "        summary_html += df.head(3).to_html()\n",
    "        summary_html += \"</details><br>\"\n",
    "    \n",
    "    return HTML(summary_html)\n",
    "\n",
    "create_data_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_search_results(search_term, output_file='search_results.csv'):\n",
    "    \"\"\"Export all search results to a CSV file\"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for table_name, df in dataframes.items():\n",
    "        mask = pd.Series([False] * len(df))\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                mask |= df[col].astype(str).str.contains(search_term, na=False, case=False)\n",
    "        \n",
    "        results = df[mask].copy()\n",
    "        if len(results) > 0:\n",
    "            results['source_table'] = table_name\n",
    "            all_results.append(results)\n",
    "    \n",
    "    if all_results:\n",
    "        combined = pd.concat(all_results, ignore_index=True)\n",
    "        combined.to_csv(output_file, index=False)\n",
    "        print(f\"Exported {len(combined)} results to {output_file}\")\n",
    "        return combined\n",
    "    else:\n",
    "        print(\"No results found\")\n",
    "        return None\n",
    "\n",
    "# Example: Export all SEED compound references\n",
    "# export_search_results('seed.compound', 'seed_compounds.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}